# Production-Ready Agentic RAG System

A complete, production-grade Retrieval-Augmented Generation (RAG) system built with LangChain, LangGraph, and open-source models. This system features an autonomous agentic workflow capable of self-reflection, query decomposition, and answer refinement.

## ğŸš€ Features

### Core Capabilities
- **Agentic Workflow**: Uses LangGraph for orchestrating complex reasoning loops (Decomposition â†’ Search â†’ Reflect â†’ Refine).
- **Advanced Ingestion**: Powered by **Docling** for high-fidelity parsing of PDFs (with OCR), Office docs (DOCX, XLSX, PPTX), images, and HTML.
- **Async-First Architecture**: Celery workers handle embeddings; API responds immediately
- **Hybrid Search**: Semantic (pgvector) + BM25 (PostgreSQL FTS) with reciprocal rank fusion.
- **Advanced Reranking**: Multi-stage reranking using Cross-Encoders (BGE-Reranker) and MMR for diversity.
- **Multi-Factor Ranking**: Combines relevance, recency, specificity, citation frequency, and source quality
- **Citation Verification**: Post-generation validation to prevent hallucinations by verifying claims against source text.
- **Self-Correction**: The agent evaluates its own answers and enters a refinement loop if quality standards aren't met.
- **Hardware Adaptation**: Auto-detects NVIDIA/AMD GPUs or Apple Metal to optimize model loading.
- **Content Deduplication**: SHA-256 hashing prevents re-processing identical documents

### Tech Stack
- **Backend**: FastAPI + Python 3.12+
- **Database**: PostgreSQL 16 + pgvector (with async SQLAlchemy)
- **Vector Search**: HNSW indexing for <200ms searches
- **Task Queue**: Celery + Redis
- **LLM Support**: 
  - **Ollama** (Local inference)
  - **Google Vertex AI** (Gemini models)
  - **vLLM** (High-throughput serving)
- **Ingestion**: Docling (IBM) for layout-aware document parsing
- **Embeddings**: sentence-transformers/all-mpnet-base-v2 (768-dim)
- **Orchestration**: LangGraph state machines
- **Frontend**: React 18 + TypeScript + Vite + Tailwind
## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- 8GB+ RAM (16GB recommended)
- Optional: NVIDIA GPU with CUDA support for faster inference

## ğŸ› ï¸ Quick Start

### 1. Clone and Setup

```bash
# Clone repository
git clone <your-repo-url>
cd agentic-rag-system

# Run the automated setup script
chmod +x quick_setup.sh
./quick_setup.sh

cd backend

# Create environment file
cp .env.example .env

# Edit .env with your settings
nano .env
```

### 2. Environment Configuration

```bash
# .env
# Database & Queue
DATABASE_URL=postgresql://postgres:password@localhost:5432/rag_db
REDIS_URL=redis://localhost:6379/0
GEMINI_API_KEY=your_gemini_key_optional

# LLM Selection
OLLAMA_BASE_URL=http://localhost:11434
# To use Google Vertex AI (Gemini):
GOOGLE_APPLICATION_CREDENTIALS=./backend/vertex.json
GOOGLE_CLOUD_PROJECT=your-project-id
GOOGLE_CLOUD_LOCATION=us-central1
# To use vLLM:
VLLM_BASE_URL=http://your-vllm-instance:8000

# Hardware settings
ENABLE_GPU=auto 
FORCE_CPU=false  # Set to true to force CPU-only mode

# Model configuration
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
CHAT_MODEL=mistral:7b-instruct-q4_0
```

### 3. Environment Setup
```bash
cd backend
uv venv
uv pip install -r requirements.txt
cd..
cd frontend
npm i
```

### 4. Launch Stack Locally

```bash
# Start backend services
cd backend
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload         

# Start frontend services
cd frontend
npm run dev
```

### 4. Launch Stack through Docker

```bash
# Start all services
docker-compose up -d

# Wait for models to download (first run only, ~2GB)
docker-compose logs -f ollama

# Once Ollama is ready, pull the model
docker exec -it rag_ollama ollama pull mistral:7b-instruct-q4_0
```

### 5. Verify Services

```bash
# Check all services are healthy
docker-compose ps

# Test API
curl http://localhost:8000/api/health

# View logs
docker-compose logs -f backend
docker-compose logs -f celery_worker
```

### 6. Access Application

- **Frontend**: http://localhost:3000
- **API Docs**: http://localhost:8000/docs
- **API Base**: http://localhost:8000/api

## ğŸ“ Project Structure

```
agentic-rag-system/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”‚   â”œâ”€â”€ database.py             # DB connection
â”‚   â”‚   â”œâ”€â”€ api/                    # REST endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ resources.py        # Document upload
â”‚   â”‚   â”‚   â”œâ”€â”€ conversations.py    # Chat endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ workspaces.py    # Workspace endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py           # Health checks
â”‚   â”‚   â”œâ”€â”€ services/               # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py        # Document parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.py        # Vector generation
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_rag.py     # Reranking & Query Decomposition
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py           # Hybrid search
â”‚   â”‚   â”‚   â”œâ”€â”€ ranking.py          # Scoring logic
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py      # LLM abstraction
â”‚   â”‚   â”‚   â”œâ”€â”€ citation.py         # Citation verification
â”‚   â”‚   â”‚   â””â”€â”€ hardware.py         # GPU/CPU detection
â”‚   â”‚   â”œâ”€â”€ agents/                 # LangGraph agents
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_agent.py        # Main RAG orchestration
â”‚   â”‚   â”‚   â””â”€â”€ tools.py            # LangChain tools
â”‚   â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py          # Pydantic models
â”‚   â”‚   â”‚   â””â”€â”€ database_models.py  # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ tasks/                  # Celery tasks
â”‚   â”‚       â”œâ”€â”€ celery_app.py       # Celery config
â”‚   â”‚       â””â”€â”€ embedding_tasks.py  # Async workers
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentUpload.tsx
â”‚   â”‚   â”‚   â””â”€â”€ SourceCitations.tsx
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ client.ts           # API client
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ init.sql                    # Schema + pgvector
â”‚   â””â”€â”€ migrations/
â”œâ”€â”€ frontend/
â”œâ”€â”€ docker-compose.yml              # 7-service stack
â””â”€â”€ README.md
```

## ğŸ”§ API Usage

### Create Workspace

```bash
curl -X POST "http://localhost:8000/api/workspaces/" \
  -H "Content-Type: application/json" \
  -d '{"name": "Research Project", "workspace_type": "personal"}'
```

### Upload Documents
Supports PDF, DOCX, XLSX, PPTX, Images, etc.
```bash
curl -X POST "http://localhost:8000/api/resources/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@paper.pdf" \
  -F "workspace_id=<workspace-uuid>"
```

### Check Embedding Status

```bash
curl "http://localhost:8000/api/resources/<resource-id>/embedding-status"
```

### Stream Chat Response

```bash
curl -X POST "http://localhost:8000/api/conversations/<conv-id>/messages" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Analyze the revenue growth mentioned in the documents.",
    "workspace_id": "<workspace-uuid>"
  }'
```

## ğŸ¯ RAG Pipeline Flow

```
1. Query Expansion
   â”œâ”€> Generate 3-5 query variants
   â””â”€> Include synonyms and reformulations

2. Hybrid Search
   â”œâ”€> Hybrid Search (Vector + Keyword)
   â”œâ”€> Advanced Reranking (Cross-Encoder / Hybrid Score)
   â””â”€> Diversity Check (MMR)

3. Context Assembly
   â”œâ”€> Token Budgeting (allocate tokens for primary vs supporting sources)
   â””â”€> Metadata Injection (recency, source quality)
4. LLM Generation
   â””â”€> LLM generates initial answer 
   â””â”€> Strict citation requirements

5. Verification & Reflection
   â”œâ”€> Verify Citations: Check if [Source X] actually supports the claim
   â”œâ”€> Self-Reflection: LLM grades its own answer (0-10 score)
   â”‚    â””â”€> If Score < 6: Loop back to Refinement
   â””â”€> Refine Response: Fix missing citations or logic gaps
```

### Database Optimization

```sql
-- Adjust HNSW index parameters for your workload
CREATE INDEX idx_chunks_embedding ON chunks 
USING hnsw (embedding vector_cosine_ops)
WITH (
  m = 16,              -- Connections per layer (higher = better recall, more memory)
  ef_construction = 64 -- Build quality (higher = better index, slower build)
);

-- Query-time tuning
SET hnsw.ef_search = 100;  -- Higher = better recall, slower search
```

### Celery Workers

```bash
# Adjust concurrency based on CPU cores
# In docker-compose.yml, celery_worker service:
celery -A app.tasks.celery_app worker \         
  --loglevel=info \
  --pool=solo \
  -Q embeddings
```

### Embedding Batch Size

```python
# Automatically adjusted based on hardware
# Manual override in config.py:
BATCH_SIZE = 64  # GPU with 16GB+ RAM
BATCH_SIZE = 32  # GPU with 8GB RAM
BATCH_SIZE = 16  # CPU mode
```

## ğŸ› Troubleshooting

### Ollama Model Not Found

```bash
docker exec -it rag_ollama ollama pull gemma3:4b
```

### Out of Memory

```bash
# Reduce batch size in .env
BATCH_SIZE=16

# Use smaller model
CHAT_MODEL=gemma3:1b

# Force CPU mode
FORCE_CPU=true
```

### Slow Searches

```sql
-- Rebuild HNSW index
REINDEX INDEX CONCURRENTLY idx_chunks_embedding;

-- Or adjust ef_search
SET hnsw.ef_search = 50;  -- Lower = faster, less accurate
```

### Celery Tasks Stuck

```bash
# Restart worker
docker-compose restart celery_worker

# Purge queue
docker exec -it rag_redis redis-cli FLUSHDB
```

## ğŸ“ˆ Monitoring


### Logs

```bash
# Backend logs
docker-compose logs -f backend

# Celery worker logs
docker-compose logs -f celery_worker

# Database logs
docker-compose logs -f postgres
```

### Metrics

Access Celery Flower (optional):
```bash
docker run -p 5555:5555 mher/flower:latest \
  --broker=redis://localhost:6379/0
```

## ğŸ”’ Security Considerations

1. **Change default passwords** in `.env`
2. **Use HTTPS** in production
3. **Enable authentication** for API endpoints
4. **Restrict CORS origins** in `config.py`
6. **Rate limit** API endpoints
7. **Sanitize user inputs** to prevent injection attacks

## ğŸ“ License

MIT License - see LICENSE file for details.


## ğŸ“š References

- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Guide](https://langchain-ai.github.io/langgraph/)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Ollama Models](https://ollama.ai/library)
- [Sentence Transformers](https://www.sbert.net/)
- [Docling](https://docling-project.github.io/docling/)