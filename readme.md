# Production-Ready Agentic RAG System

A complete, production-grade Retrieval-Augmented Generation (RAG) system built with LangChain, LangGraph, and open-source models.

## ğŸš€ Features

### Core Capabilities
- **Async-First Architecture**: Celery workers handle embeddings; API responds immediately
- **Hybrid Search**: Semantic + BM25 with reciprocal rank fusion
- **Multi-Factor Ranking**: Combines relevance, recency, specificity, citation frequency, and source quality
- **Citation Verification**: Post-generation validation of LLM claims against sources
- **Hardware Adaptation**: Auto-detects GPU/CPU, adjusts models and timeouts
- **Content Deduplication**: SHA-256 hashing prevents re-processing identical documents

### Tech Stack
- **Backend**: FastAPI + Python 3.11+
- **Database**: PostgreSQL 16 + pgvector
- **Vector Search**: HNSW indexing for <200ms searches
- **Task Queue**: Celery + Redis
- **LLM**: Ollama (Mistral 7B 4-bit quantized) or Google Gemini
- **Embeddings**: sentence-transformers/all-mpnet-base-v2 (768-dim)
- **Orchestration**: LangGraph for agentic workflows
- **Frontend**: React 19 + TypeScript + Vite

## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- 8GB+ RAM (16GB recommended)
- Optional: NVIDIA GPU with CUDA support for faster inference

## ğŸ› ï¸ Quick Start

### 1. Clone and Setup

```bash
# Clone repository
git clone <your-repo-url>
cd agentic-rag-system

# Create environment file
cp .env.example .env

# Edit .env with your settings
nano .env
```

### 2. Environment Configuration

```bash
# .env
POSTGRES_PASSWORD=your_secure_password
GEMINI_API_KEY=your_gemini_key_optional

# Hardware settings
ENABLE_GPU=auto  # auto, true, or false
FORCE_CPU=false  # Set to true to force CPU-only mode

# Model configuration
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
CHAT_MODEL=mistral:7b-instruct-q4_0
```

### 3. Launch Stack

```bash
# Start all services
docker-compose up -d

# Wait for models to download (first run only, ~2GB)
docker-compose logs -f ollama

# Once Ollama is ready, pull the model
docker exec -it rag_ollama ollama pull mistral:7b-instruct-q4_0
```

### 4. Verify Services

```bash
# Check all services are healthy
docker-compose ps

# Test API
curl http://localhost:8000/api/health

# View logs
docker-compose logs -f backend
docker-compose logs -f celery_worker
```

### 5. Access Application

- **Frontend**: http://localhost:3000
- **API Docs**: http://localhost:8000/docs
- **API Base**: http://localhost:8000/api

## ğŸ“ Project Structure

```
agentic-rag-system/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”‚   â”œâ”€â”€ database.py             # DB connection
â”‚   â”‚   â”œâ”€â”€ api/                    # REST endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ resources.py        # Document upload
â”‚   â”‚   â”‚   â”œâ”€â”€ conversations.py    # Chat endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py           # Health checks
â”‚   â”‚   â”œâ”€â”€ services/               # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py        # Document parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.py        # Vector generation
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py           # Hybrid search
â”‚   â”‚   â”‚   â”œâ”€â”€ ranking.py          # Multi-factor ranking
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py      # LLM abstraction
â”‚   â”‚   â”‚   â”œâ”€â”€ citation.py         # Citation verification
â”‚   â”‚   â”‚   â””â”€â”€ hardware.py         # GPU/CPU detection
â”‚   â”‚   â”œâ”€â”€ agents/                 # LangGraph agents
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_agent.py        # Main RAG orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ tools.py            # LangChain tools
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py          # Prompt templates
â”‚   â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py          # Pydantic models
â”‚   â”‚   â”‚   â””â”€â”€ database_models.py  # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ tasks/                  # Celery tasks
â”‚   â”‚       â”œâ”€â”€ celery_app.py       # Celery config
â”‚   â”‚       â””â”€â”€ embedding_tasks.py  # Async workers
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentUpload.tsx
â”‚   â”‚   â”‚   â””â”€â”€ SourceCitations.tsx
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ client.ts           # API client
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ init.sql                    # Schema + pgvector
â”‚   â””â”€â”€ migrations/
â”œâ”€â”€ docker-compose.yml              # 7-service stack
â””â”€â”€ README.md
```

## ğŸ”§ API Usage

### Upload Documents

```bash
curl -X POST "http://localhost:8000/api/resources/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf" \
  -F "workspace_id=<workspace-uuid>"
```

### Check Embedding Status

```bash
curl "http://localhost:8000/api/resources/<resource-id>/embedding-status"
```

### Send Chat Message

```bash
curl -X POST "http://localhost:8000/api/conversations/<conv-id>/messages" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "What are the key findings?",
    "workspace_id": "<workspace-uuid>"
  }'
```

## ğŸ¯ RAG Pipeline Flow

```
1. Query Expansion
   â”œâ”€> Generate 3-5 query variants
   â””â”€> Include synonyms and reformulations

2. Hybrid Search
   â”œâ”€> Semantic search (pgvector cosine similarity)
   â”œâ”€> BM25 keyword search (PostgreSQL FTS)
   â””â”€> Reciprocal rank fusion merge

3. Multi-Factor Ranking
   â”œâ”€> Base relevance (40%)
   â”œâ”€> Citation frequency (15%)
   â”œâ”€> Recency (15%)
   â”œâ”€> Specificity (15%)
   â””â”€> Source quality (15%)

4. Context Assembly
   â”œâ”€> Token budget: 2000 default
   â”œâ”€> Primary sources: 60%
   â”œâ”€> Supporting context: 30%
   â””â”€> Metadata: 10%

5. LLM Generation
   â”œâ”€> Streaming response
   â”œâ”€> Strict citation requirements
   â””â”€> Anti-hallucination prompts

6. Citation Verification
   â”œâ”€> Extract [Source N] references
   â”œâ”€> Validate against source chunks
   â””â”€> Flag mismatches
```

## ğŸ” Hardware Optimization

### GPU Detection

The system automatically detects and optimizes for:
- **NVIDIA GPUs**: Uses CUDA acceleration
- **AMD GPUs**: Uses ROCm support
- **Apple Silicon**: Uses Metal acceleration
- **CPU-only**: Adjusts batch sizes and timeouts

### Model Selection

| Hardware | Model |
|----------|-------|
| GPU + 16GB+ RAM | mistral:7b-instruct |
| GPU + 8GB RAM | mistral:7b-instruct-q4_0 |
| CPU + 16GB RAM | mistral:7b-instruct-q4_0 |
| CPU + 8GB RAM | gemma:2b-instruct-q4_0 |
| CPU + <8GB RAM | phi:2.7b-instruct-q4_0 |

## ğŸ“Š Performance Tuning

### Database Optimization

```sql
-- Adjust HNSW index parameters for your workload
CREATE INDEX idx_chunks_embedding ON chunks 
USING hnsw (embedding vector_cosine_ops)
WITH (
  m = 16,              -- Connections per layer (higher = better recall, more memory)
  ef_construction = 64 -- Build quality (higher = better index, slower build)
);

-- Query-time tuning
SET hnsw.ef_search = 100;  -- Higher = better recall, slower search
```

### Celery Workers

```bash
# Adjust concurrency based on CPU cores
# In docker-compose.yml, celery_worker service:
command: celery -A app.tasks.celery_app worker --loglevel=info --concurrency=4
```

### Embedding Batch Size

```python
# Automatically adjusted based on hardware
# Manual override in config.py:
BATCH_SIZE = 64  # GPU with 16GB+ RAM
BATCH_SIZE = 32  # GPU with 8GB RAM
BATCH_SIZE = 16  # CPU mode
```

## ğŸ› Troubleshooting

### Ollama Model Not Found

```bash
docker exec -it rag_ollama ollama pull mistral:7b-instruct-q4_0
```

### Out of Memory

```bash
# Reduce batch size in .env
BATCH_SIZE=16

# Use smaller model
CHAT_MODEL=phi:2.7b-instruct-q4_0

# Force CPU mode
FORCE_CPU=true
```

### Slow Searches

```sql
-- Rebuild HNSW index
REINDEX INDEX CONCURRENTLY idx_chunks_embedding;

-- Or adjust ef_search
SET hnsw.ef_search = 50;  -- Lower = faster, less accurate
```

### Celery Tasks Stuck

```bash
# Restart worker
docker-compose restart celery_worker

# Purge queue
docker exec -it rag_redis redis-cli FLUSHDB
```

## ğŸ“ˆ Monitoring

### Health Checks

```bash
# Overall system health
curl http://localhost:8000/api/health

# Database connection
curl http://localhost:8000/api/health/db

# Ollama status
curl http://localhost:11434/api/tags
```

### Logs

```bash
# Backend logs
docker-compose logs -f backend

# Celery worker logs
docker-compose logs -f celery_worker

# Database logs
docker-compose logs -f postgres
```

### Metrics

Access Celery Flower (optional):
```bash
docker run -p 5555:5555 mher/flower:latest \
  --broker=redis://localhost:6379/0
```

## ğŸ”’ Security Considerations

1. **Change default passwords** in `.env`
2. **Use HTTPS** in production
3. **Enable authentication** for API endpoints
4. **Restrict CORS origins** in `config.py`
5. **Scan uploaded files** for malware
6. **Rate limit** API endpoints
7. **Sanitize user inputs** to prevent injection attacks

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please read CONTRIBUTING.md first.

## ğŸ“š References

- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Guide](https://langchain-ai.github.io/langgraph/)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Ollama Models](https://ollama.ai/library)
- [Sentence Transformers](https://www.sbert.net/)

## ğŸ’¬ Support

- Issues: GitHub Issues
- Discussions: GitHub Discussions
- Email: support@example.com